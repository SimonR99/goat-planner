{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, IA3Config\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import bitsandbytes as bnb\n",
    "from enum import Enum\n",
    "import wandb\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "from trl import SFTConfig\n",
    "\n",
    "\n",
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSize(Enum):\n",
    "    LLAMA_1B = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    LLAMA_3B = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "class TrainingMethod(Enum):\n",
    "    LORA = \"lora\"\n",
    "    IA3 = \"ia3\"\n",
    "\n",
    "class OutputFormat(Enum):\n",
    "    XML = \"xml\"\n",
    "    JSON = \"json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    model_size: ModelSize\n",
    "    training_method: TrainingMethod\n",
    "    output_format: OutputFormat\n",
    "    num_epochs: int = 10\n",
    "    learning_rate: float = 2e-4\n",
    "    batch_size: int = 1\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    max_seq_length: int = 900\n",
    "\n",
    "def setup_model_and_tokenizer(model_size: ModelSize):\n",
    "    \"\"\"Initialize model and tokenizer with appropriate configuration\"\"\"\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\"\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_size.value,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=attn_implementation\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_size.value, trust_remote_code=True)\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    \"\"\"Find all linear layer names in the model for LoRA configuration\"\"\"\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "def get_peft_config(method: TrainingMethod, model):\n",
    "    \"\"\"Get the appropriate PEFT configuration based on training method\"\"\"\n",
    "    if method == TrainingMethod.LORA:\n",
    "        return LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            target_modules=find_all_linear_names(model)\n",
    "        )\n",
    "    else:  # IA3\n",
    "        return IA3Config(\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            target_modules=[\"k_proj\", \"v_proj\", \"down_proj\"],\n",
    "            feedforward_modules=[\"down_proj\"]\n",
    "        )\n",
    "\n",
    "def formatting_prompt(examples):\n",
    "    llama_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "{}<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "{}\n",
    "\"\"\"\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction,input, output in zip(instructions, inputs, outputs):\n",
    "        text = llama_prompt.format(instruction,input, output)\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "def prepare_training_data(output_format: OutputFormat):\n",
    "    \"\"\"Prepare training data based on output format\"\"\"\n",
    "    # Load your data\n",
    "    data = pd.read_json(\"../data/behavior_tree_dataset.json\")\n",
    "    \n",
    "    # Extract relevant columns\n",
    "    object_context = data['object_context']\n",
    "    action_context = data['actions_dictionary']\n",
    "    query = data['query']\n",
    "    bt_xml = data['bt_xml']\n",
    "    bt_json = data['bt_json']\n",
    "\n",
    "    # Load templates\n",
    "    with open('../data/templates.json', 'r') as f:\n",
    "        templates = json.load(f)\n",
    "    \n",
    "    # Load actions\n",
    "    with open('../data/actions.json', 'r') as f:\n",
    "        actions = json.load(f)\n",
    "    \n",
    "    # Load objects\n",
    "    with open('../data/objects.json', 'r') as f:\n",
    "        objects = json.load(f)\n",
    "\n",
    "    template = templates['template']\n",
    "    action_list = actions['action_list']\n",
    "    object_list = objects['object_list']\n",
    "    question_example = templates['question_example']\n",
    "    xml_example = templates['xml_example']\n",
    "    json_example = templates['json_example']\n",
    "    answer_example = templates['answer_example']\n",
    "    training_template = templates['training_template']\n",
    "\n",
    "    # Format data based on output format\n",
    "    if output_format == OutputFormat.XML:\n",
    "        system_template = template.format(\n",
    "            format_type=\"XML\",\n",
    "            example=question_example + \"\\n\" + answer_example + \"\\n\" + xml_example,\n",
    "            available_actions=action_list,\n",
    "            object_list=object_list,\n",
    "        )\n",
    "        output_data = bt_xml.apply(lambda x: f\"<plan>{x}</plan>\")\n",
    "    else:\n",
    "        system_template = template.format(\n",
    "            format_type=\"JSON\",\n",
    "            example=question_example + \"\\n\" + answer_example + \"\\n\" + json_example,\n",
    "            available_actions=action_list,\n",
    "            object_list=object_list,\n",
    "        )\n",
    "        output_data = bt_json.apply(lambda x: f\"<plan>{x}</plan>\")\n",
    "    \n",
    "    training_systems = [\n",
    "        training_template.format(\n",
    "            available_actions=action_list,\n",
    "            object_list=object_list,\n",
    "        ) for object_list in object_context]\n",
    "\n",
    "    formatted_data = pd.DataFrame({\n",
    "        'complete_instruction': system_template,\n",
    "        'instruction': training_systems,\n",
    "        'input': query,\n",
    "        'output': output_data,\n",
    "    })\n",
    "\n",
    "    # Return the formatted data (only first 20 examples)\n",
    "    formatted_data = formatted_data.head(20)\n",
    "\n",
    "    # Print first training example output column\n",
    "    print(formatted_data.iloc[0]['output'])\n",
    "\n",
    "    return Dataset.from_pandas(formatted_data).map(formatting_prompt, batched=True)\n",
    "\n",
    "def setup_trainer(model, tokenizer, training_data, config: TrainingConfig):\n",
    "    \"\"\"Setup the trainer with appropriate configuration\"\"\"\n",
    "    model_name = f\"llama-{config.model_size.value.split('-')[-2]}-bt-{config.output_format.value}-{config.training_method.value}\"\n",
    "    \n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=model_name,\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        per_device_eval_batch_size=config.batch_size,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        num_train_epochs=config.num_epochs if config.training_method == TrainingMethod.IA3 else 1,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=0.1,\n",
    "        logging_steps=1,\n",
    "        warmup_steps=10,\n",
    "        learning_rate=1e-4 if config.training_method == TrainingMethod.IA3 else config.learning_rate,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        group_by_length=True,\n",
    "        report_to=\"wandb\",\n",
    "    )\n",
    "\n",
    "    full_dataset = training_data.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "    sft_config = SFTConfig(\n",
    "        max_seq_length=config.max_seq_length,\n",
    "        packing=False,\n",
    "        **training_arguments.to_dict()\n",
    "    )\n",
    "\n",
    "\n",
    "    return SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=full_dataset[\"train\"],\n",
    "        eval_dataset=full_dataset[\"test\"],\n",
    "        peft_config=get_peft_config(config.training_method, model),\n",
    "        args=sft_config,\n",
    "        processing_class=tokenizer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimonroy99\u001b[0m (\u001b[33msimonroy99-cole-de-technologie-sup-rieure\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\workspace\\ETS\\SYS819\\goat-planner\\training\\wandb\\run-20241220_134440-sbjk6omj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/simonroy99-cole-de-technologie-sup-rieure/bt-training/runs/sbjk6omj' target=\"_blank\">tough-terrain-5</a></strong> to <a href='https://wandb.ai/simonroy99-cole-de-technologie-sup-rieure/bt-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/simonroy99-cole-de-technologie-sup-rieure/bt-training' target=\"_blank\">https://wandb.ai/simonroy99-cole-de-technologie-sup-rieure/bt-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/simonroy99-cole-de-technologie-sup-rieure/bt-training/runs/sbjk6omj' target=\"_blank\">https://wandb.ai/simonroy99-cole-de-technologie-sup-rieure/bt-training/runs/sbjk6omj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<plan><root main_tree_to_execute=\"LocateAndNavigateToLadderSequence\">\n",
      "    <BehaviorTree ID=\"LocateAndNavigateToLadderSequence\">\n",
      "        <Sequence name=\"LocateAndNavigateToLadder\">\n",
      "            <Retry num_attempts=\"3\">\n",
      "                <Locate object=\"cooking pot\" \n",
      "                        position_x=\"{cooking_pot_position_x}\" \n",
      "                        position_y=\"{cooking_pot_position_y}\" \n",
      "                        position_z=\"{cooking_pot_position_z}\" />\n",
      "            </Retry>\n",
      "            <Fallback name=\"HandleCookingPotFallback\">\n",
      "                <Sequence name=\"NavigateAfterLocatingPot\">\n",
      "                    <Retry num_attempts=\"2\">\n",
      "                        <Navigate x=\"{ladder_position_x}\" y=\"{ladder_position_y}\" />\n",
      "                    </Retry>\n",
      "                </Sequence>\n",
      "                <Sequence name=\"FailedToLocatePotHandling\">\n",
      "                    <Wait duration=\"2.0\" />\n",
      "                    <Navigate x=\"{ladder_position_x}\" y=\"{ladder_position_y}\" />\n",
      "                </Sequence>\n",
      "            </Fallback>\n",
      "        </Sequence>\n",
      "    </BehaviorTree>\n",
      "</root></plan>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16335a06ce474eb1ad910763c9751c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\workspace\\ETS\\SYS819\\goat-planner\\env\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\workspace\\ETS\\SYS819\\goat-planner\\env\\Lib\\site-packages\\transformers\\training_args.py:2070: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d600ec1bef6d47eab340e327a5c9da0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e8d01de9174a6fbec856837361d42f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a14ff272394bb38f4ccad47da4fe93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4355, 'grad_norm': 1.2515957355499268, 'learning_rate': 2e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227d4369b61d4da5a11d00c884f08824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4686474800109863, 'eval_runtime': 0.3487, 'eval_samples_per_second': 5.735, 'eval_steps_per_second': 5.735, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1542, 'grad_norm': 1.0203007459640503, 'learning_rate': 4e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89dfdabc980c4772b010b30c1ad0e7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4441306591033936, 'eval_runtime': 0.3415, 'eval_samples_per_second': 5.857, 'eval_steps_per_second': 5.857, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4437, 'grad_norm': 1.2312090396881104, 'learning_rate': 6e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a4257a6fee4e72923c08d2f29e53bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3912301063537598, 'eval_runtime': 0.342, 'eval_samples_per_second': 5.848, 'eval_steps_per_second': 5.848, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3792, 'grad_norm': 1.2149678468704224, 'learning_rate': 8e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f0d4dc0bd54a68b989b58432fa9b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3182319402694702, 'eval_runtime': 0.3395, 'eval_samples_per_second': 5.89, 'eval_steps_per_second': 5.89, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3153, 'grad_norm': 1.0672197341918945, 'learning_rate': 0.0001, 'epoch': 0.56}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c0016314724eeda24e26ab3dd220c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2337559461593628, 'eval_runtime': 0.3443, 'eval_samples_per_second': 5.81, 'eval_steps_per_second': 5.81, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2065, 'grad_norm': 0.9875402450561523, 'learning_rate': 0.00012, 'epoch': 0.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5752134e645f4bb5af962ad82d6bb7fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1360883712768555, 'eval_runtime': 0.3431, 'eval_samples_per_second': 5.829, 'eval_steps_per_second': 5.829, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9277, 'grad_norm': 0.7119359970092773, 'learning_rate': 0.00014, 'epoch': 0.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba7e3d198154a43b05591bf2ba55e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0340609550476074, 'eval_runtime': 0.3425, 'eval_samples_per_second': 5.839, 'eval_steps_per_second': 5.839, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0344, 'grad_norm': 0.9036762714385986, 'learning_rate': 0.00016, 'epoch': 0.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc12bb163ba24a97a3dd20b1a1c1c58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9266351461410522, 'eval_runtime': 0.3627, 'eval_samples_per_second': 5.515, 'eval_steps_per_second': 5.515, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8899, 'grad_norm': 0.828197717666626, 'learning_rate': 0.00018, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9837a9331d40249236796ad59033a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8178315162658691, 'eval_runtime': 0.3567, 'eval_samples_per_second': 5.607, 'eval_steps_per_second': 5.607, 'epoch': 1.0}\n",
      "{'train_runtime': 11.3162, 'train_samples_per_second': 1.591, 'train_steps_per_second': 0.795, 'train_loss': 1.1984891162978277, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Initialize wandb\n",
    "wandb.init(project=\"bt-training\")\n",
    "\n",
    "# Training configuration\n",
    "config = TrainingConfig(\n",
    "    model_size=ModelSize.LLAMA_1B,\n",
    "    training_method=TrainingMethod.LORA,\n",
    "    output_format=OutputFormat.XML,\n",
    ")\n",
    "\n",
    "# Setup\n",
    "model, tokenizer = setup_model_and_tokenizer(config.model_size)\n",
    "training_data = prepare_training_data(config.output_format)\n",
    "trainer = setup_trainer(model, tokenizer, training_data, config)\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from typing import Optional, Dict, Any\n",
    "import os\n",
    "from evaluation import evaluate_model\n",
    "import json\n",
    "\n",
    "def load_trained_model(\n",
    "    base_model_name: str,\n",
    "    adapter_path: str,\n",
    "    device: str = \"auto\"\n",
    ") -> tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"\n",
    "    Load a trained LoRA model and its tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: Name of the base model\n",
    "        adapter_path: Path to the trained adapter weights\n",
    "        device: Device to load the model on\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    \n",
    "    # Setup base model with same config as training\n",
    "    torch_dtype = torch.float16\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    # Load base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=device,\n",
    "        torch_dtype=torch_dtype\n",
    "    )\n",
    "    \n",
    "    # Load adapter weights\n",
    "    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def evaluate_all_models(\n",
    "    model_configs: list[Dict[str, Any]],\n",
    "    template: str,\n",
    "    question_example: str,\n",
    "    answer_example: str,\n",
    "    json_example: str,\n",
    "    xml_example: str,\n",
    "    action_list: list[str],\n",
    "    object_list: list[str],\n",
    "    query_file: str,\n",
    "    evaluate_model_fn: callable\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate multiple trained models with different configurations.\n",
    "    \n",
    "    Args:\n",
    "        model_configs: List of dictionaries containing model configurations\n",
    "        template: Template string for system prompt\n",
    "        question_example: Example question\n",
    "        answer_example: Example answer\n",
    "        json_example: Example JSON output\n",
    "        action_list: List of available actions\n",
    "        object_list: List of available objects\n",
    "        query_file: Path to query dataset\n",
    "        evaluate_model_fn: Function to evaluate individual model\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing evaluation results for each model\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for config in model_configs:\n",
    "        model_name = f\"llama-{config['size']}-bt-{config['format']}-{config['method']}\"\n",
    "        adapter_path = f\"./{model_name}\"\n",
    "\n",
    "        # Check if the model exists\n",
    "        if not os.path.exists(adapter_path):\n",
    "            print(f\"Model {model_name} does not exist. Skipping evaluation.\")\n",
    "            continue\n",
    "        \n",
    "        # Load model\n",
    "        model, tokenizer = load_trained_model(\n",
    "            config['base_model'],\n",
    "            adapter_path\n",
    "        )\n",
    "        \n",
    "        # Prepare system prompt\n",
    "        if config['format'].lower() == 'xml':\n",
    "            system_prompt = template.format(\n",
    "                format_type=config['format'].upper(),\n",
    "                example=f\"{question_example}\\n{answer_example}\\n{xml_example}\",\n",
    "                available_actions=action_list,\n",
    "                object_list=object_list,\n",
    "            ) \n",
    "        else:\n",
    "            system_prompt = template.format(\n",
    "                format_type=config['format'].upper(),\n",
    "                example=f\"{question_example}\\n{answer_example}\\n{json_example}\",\n",
    "                available_actions=action_list,\n",
    "                object_list=object_list,\n",
    "            )\n",
    "\n",
    "        \n",
    "        # Evaluate model\n",
    "        results[model_name] = evaluate_model_fn(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            formatting_prompt=formatting_prompt,\n",
    "            validation_type=config['format'].lower(),\n",
    "            query_file=query_file,\n",
    "            instruction=system_prompt,\n",
    "            action_list=action_list\n",
    "        )\n",
    "        \n",
    "        # Clear CUDA cache\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model llama-1b-bt-json-lora does not exist. Skipping evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 6/50 [01:14<08:54, 12.14s/it]"
     ]
    }
   ],
   "source": [
    "# Load templates\n",
    "with open('../data/templates.json', 'r') as f:\n",
    "    templates = json.load(f)\n",
    "\n",
    "# Load actions\n",
    "with open('../data/actions.json', 'r') as f:\n",
    "    actions = json.load(f)\n",
    "\n",
    "# Load objects\n",
    "with open('../data/objects.json', 'r') as f:\n",
    "    objects = json.load(f)\n",
    "\n",
    "template = templates['template']\n",
    "action_list = actions['action_list']\n",
    "object_list = objects['object_list']\n",
    "question_example = templates['question_example']\n",
    "xml_example = templates['xml_example']\n",
    "json_example = templates['json_example']\n",
    "answer_example = templates['answer_example']\n",
    "training_template = templates['training_template']\n",
    "\n",
    "# Define configurations for all models you want to evaluate\n",
    "model_configs = [\n",
    "    {\n",
    "        'size': '1b',\n",
    "        'method': 'lora',\n",
    "        'format': 'json',\n",
    "        'base_model': ModelSize.LLAMA_1B.value\n",
    "    },\n",
    "    {\n",
    "        'size': '1b',\n",
    "        'method': 'lora',\n",
    "        'format': 'xml',\n",
    "        'base_model': ModelSize.LLAMA_1B.value\n",
    "    },\n",
    "    {\n",
    "        'size': '3b',\n",
    "        'method': 'lora',\n",
    "        'format': 'json',\n",
    "        'base_model': ModelSize.LLAMA_3B.value\n",
    "    },\n",
    "    {\n",
    "        'size': '3b',\n",
    "        'method': 'lora',\n",
    "        'format': 'xml',\n",
    "        'base_model': ModelSize.LLAMA_3B.value\n",
    "    }\n",
    "]\n",
    "\n",
    "# Evaluate all models\n",
    "results = evaluate_all_models(\n",
    "    model_configs=model_configs,\n",
    "    template=template,\n",
    "    question_example=question_example,\n",
    "    answer_example=answer_example,\n",
    "    json_example=json_example,\n",
    "    xml_example=xml_example,\n",
    "    action_list=action_list,\n",
    "    object_list=object_list,\n",
    "    query_file=\"../data/task_dataset.json\",\n",
    "    evaluate_model_fn=evaluate_model\n",
    ")\n",
    "\n",
    "# Print results\n",
    "for model_name, result in results.items():\n",
    "    print(f\"\\nResults for {model_name}:\")\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
