{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.2) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/home/simon/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/simon/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/simon/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/simon/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 3 bottles, 1 cup, 10 knifes, 1 spoon, 1 bowl, 1 mouse, 6 scissorss, 88.0ms\n",
      "Speed: 1.6ms preprocess, 88.0ms inference, 513.5ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region 1: there are many different items on the wall in the store\n",
      "Region 2: there is a small dog that is sitting on a spoon\n",
      "Skipping region 3 due to small size: 22x241\n",
      "Region 4: there is a bottle of water sitting on a table in a room\n",
      "Region 5: there is a large stack of food on a counter in a kitchen\n",
      "Skipping region 6 due to small size: 38x26\n",
      "Skipping region 7 due to small size: 24x82\n",
      "Skipping region 8 due to small size: 19x91\n",
      "Region 9: there are many pairs of scissors on the wall in the store\n",
      "Skipping region 10 due to small size: 26x79\n",
      "Region 11: there are many tools on the shelf in the store\n",
      "Skipping region 12 due to small size: 25x147\n",
      "Skipping region 13 due to small size: 24x130\n",
      "Skipping region 14 due to small size: 15x72\n",
      "Skipping region 15 due to small size: 21x91\n",
      "Region 16: there are many different types of tools on the wall\n",
      "Skipping region 17 due to small size: 14x88\n",
      "Skipping region 18 due to small size: 14x73\n",
      "Region 19: there are several different items on the wall in the store\n",
      "Region 20: there are many tools on the wall in the shop\n",
      "Skipping region 21 due to small size: 14x76\n",
      "Skipping region 22 due to small size: 26x39\n",
      "Skipping region 23 due to small size: 30x28\n",
      "Captions for detected regions: [{'bbox': array([441, 167, 467, 324]), 'caption': 'there are many different items on the wall in the store'}, {'bbox': array([474,  79, 507, 114]), 'caption': 'there is a small dog that is sitting on a spoon'}, {'bbox': array([ 38, 347,  59, 387]), 'caption': 'there is a bottle of water sitting on a table in a room'}, {'bbox': array([108, 333, 129, 382]), 'caption': 'there is a large stack of food on a counter in a kitchen'}, {'bbox': array([534, 252, 564, 321]), 'caption': 'there are many pairs of scissors on the wall in the store'}, {'bbox': array([399, 201, 427, 329]), 'caption': 'there are many tools on the shelf in the store'}, {'bbox': array([420, 167, 447, 325]), 'caption': 'there are many different types of tools on the wall'}, {'bbox': array([484, 163, 510, 300]), 'caption': 'there are several different items on the wall in the store'}, {'bbox': array([398, 202, 428, 331]), 'caption': 'there are many tools on the wall in the shop'}]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO as UltralyticsYOLO\n",
    "from transformers import pipeline\n",
    "from typing import List, Dict\n",
    "\n",
    "# Initialize YOLO model\n",
    "class YOLO:\n",
    "    def __init__(self, model_path, device=\"cpu\", confidence_threshold=0.25, nms_threshold=0.45):\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.nms_threshold = nms_threshold\n",
    "        self.model = None\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load YOLO model.\"\"\"\n",
    "        self.model = UltralyticsYOLO(self.model_path)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def detect(self, image: np.ndarray) -> List[Dict]:\n",
    "        \"\"\"Run detection on image.\"\"\"\n",
    "        results = self.model(image, conf=self.confidence_threshold, iou=self.nms_threshold)\n",
    "        return self.postprocess(results)\n",
    "    \n",
    "    def postprocess(self, output) -> List[Dict]:\n",
    "        \"\"\"Convert YOLO output to list of detections.\"\"\"\n",
    "        detections = []\n",
    "        for result in output:\n",
    "            boxes = result.boxes\n",
    "            for box in boxes:\n",
    "                detection = {\n",
    "                    'bbox': box.xyxy[0].cpu().numpy().astype(int),\n",
    "                    'confidence': box.conf.item(),\n",
    "                    'class_id': int(box.cls.item()),\n",
    "                }\n",
    "                detections.append(detection)\n",
    "        return detections\n",
    "\n",
    "# Initialize BLIP model\n",
    "blip_pipeline = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-large\", device=0)\n",
    "\n",
    "def crop_and_caption(image_path: str, yolo_model: YOLO, min_width=30, min_height=30) -> List[Dict]:\n",
    "    \"\"\"Detect bounding boxes, crop them, and generate captions using BLIP.\"\"\"\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image at {image_path} could not be loaded.\")\n",
    "    \n",
    "    # Run YOLO detection\n",
    "    detections = yolo_model.detect(image)\n",
    "    \n",
    "    captions = []\n",
    "    for i, det in enumerate(detections):\n",
    "        x1, y1, x2, y2 = det['bbox']\n",
    "        # add 10% padding to the bounding box\n",
    "        padding = 0.2\n",
    "        x1 = max(0, x1 - int(padding * (x2 - x1)))\n",
    "        x2 = min(image.shape[1], x2 + int(padding * (x2 - x1)))\n",
    "        y1 = max(0, y1 - int(padding * (y2 - y1)))\n",
    "        y2 = min(image.shape[0], y2 + int(padding * (y2 - y1)))\n",
    "\n",
    "\n",
    "        cropped_width = x2 - x1\n",
    "        cropped_height = y2 - y1\n",
    "\n",
    "  \n",
    "\n",
    "        # Crop the detected region\n",
    "        cropped_image = image[y1:y2, x1:x2]\n",
    "        # Convert cropped image to RGB and save as a temporary file for BLIP\n",
    "        cropped_image_rgb = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "        if cropped_width < min_width or cropped_height < min_height:\n",
    "            print(f\"Skipping region {i + 1} due to small size: {cropped_width}x{cropped_height}\")\n",
    "            continue\n",
    "        \n",
    "        # Convert the image to PIL format (BLIP expects a PIL Image)\n",
    "        from PIL import Image\n",
    "        pil_image = Image.fromarray(cropped_image_rgb)\n",
    "\n",
    "        # Generate caption\n",
    "        caption = blip_pipeline(pil_image)[0]['generated_text']\n",
    "        captions.append({'bbox': det['bbox'], 'caption': caption})\n",
    "        \n",
    "        # For debugging, show cropped regions and captions\n",
    "        print(f\"Region {i + 1}: {caption}\")\n",
    "        cv2.imshow(f\"Cropped Region {i + 1}\", cropped_image)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    return captions\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to YOLO model\n",
    "    yolo_model_path = \"yolov8s-world.pt\"  # Replace with your YOLO model path\n",
    "    image_path = \"image.png\"  # Replace with your image path\n",
    "\n",
    "    # Initialize and load YOLO model\n",
    "    yolo = YOLO(model_path=yolo_model_path, device=\"cuda\", confidence_threshold=0.25)\n",
    "    yolo.load_model()\n",
    "\n",
    "    # Perform detection and BLIP captioning\n",
    "    results = crop_and_caption(image_path, yolo)\n",
    "    print(\"Captions for detected regions:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.2) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/home/simon/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/simon/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/simon/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/simon/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 3 bottles, 2 knifes, 1 bowl, 2 scissorss, 162.0ms\n",
      "Speed: 2.4ms preprocess, 162.0ms inference, 599.7ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region 1: a workbench with tools and tools on it\n",
      "Region 2: a workbench with tools and tools hanging on a wall\n",
      "Region 3: a workbench with tools and tools hanging on a wall\n",
      "Region 4: a workbench with tools and a lamp\n",
      "Region 5: a workbench with tools and a lamp\n",
      "Region 6: a workbench with tools and tools hanging on a wall\n",
      "Region 7: a workbench with tools and tools hanging on a wall\n",
      "Region 8: a workbench with tools and tools hanging on a wall\n",
      "Captions for detected regions: [{'bbox': array([441, 167, 467, 324]), 'caption': 'a workbench with tools and tools on it'}, {'bbox': array([474,  79, 507, 114]), 'caption': 'a workbench with tools and tools hanging on a wall'}, {'bbox': array([470, 163, 486, 331]), 'caption': 'a workbench with tools and tools hanging on a wall'}, {'bbox': array([ 38, 347,  59, 387]), 'caption': 'a workbench with tools and a lamp'}, {'bbox': array([108, 333, 129, 382]), 'caption': 'a workbench with tools and a lamp'}, {'bbox': array([ 12, 362,  39, 381]), 'caption': 'a workbench with tools and tools hanging on a wall'}, {'bbox': array([221, 314, 238, 372]), 'caption': 'a workbench with tools and tools hanging on a wall'}, {'bbox': array([365,  94, 379, 158]), 'caption': 'a workbench with tools and tools hanging on a wall'}]\n"
     ]
    }
   ],
   "source": [
    "    import cv2\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from ultralytics import YOLO as UltralyticsYOLO\n",
    "    from transformers import pipeline\n",
    "    from typing import List, Dict\n",
    "    from PIL import Image\n",
    "\n",
    "    # Initialize YOLO model\n",
    "    class YOLO:\n",
    "        def __init__(self, model_path, device=\"cpu\", confidence_threshold=0.25, nms_threshold=0.45):\n",
    "            self.model_path = model_path\n",
    "            self.device = device\n",
    "            self.confidence_threshold = confidence_threshold\n",
    "            self.nms_threshold = nms_threshold\n",
    "            self.model = None\n",
    "\n",
    "        def load_model(self):\n",
    "            \"\"\"Load YOLO model.\"\"\"\n",
    "            self.model = UltralyticsYOLO(self.model_path)\n",
    "            self.model.to(self.device)\n",
    "\n",
    "        def detect(self, image: np.ndarray) -> List[Dict]:\n",
    "            \"\"\"Run detection on image.\"\"\"\n",
    "            results = self.model(image, conf=self.confidence_threshold, iou=self.nms_threshold)\n",
    "            return self.postprocess(results)\n",
    "        \n",
    "        def postprocess(self, output) -> List[Dict]:\n",
    "            \"\"\"Convert YOLO output to list of detections.\"\"\"\n",
    "            detections = []\n",
    "            for result in output:\n",
    "                boxes = result.boxes\n",
    "                for box in boxes:\n",
    "                    detection = {\n",
    "                        'bbox': box.xyxy[0].cpu().numpy().astype(int),\n",
    "                        'confidence': box.conf.item(),\n",
    "                        'class_id': int(box.cls.item()),\n",
    "                    }\n",
    "                    detections.append(detection)\n",
    "            return detections\n",
    "\n",
    "    # Initialize BLIP model\n",
    "    blip_pipeline = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\", device=0)\n",
    "\n",
    "    def fade_and_caption(image_path: str, yolo_model: YOLO):\n",
    "        \"\"\"Detect bounding boxes, create images with faded backgrounds, and generate captions using BLIP.\"\"\"\n",
    "        # Load the image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Image at {image_path} could not be loaded.\")\n",
    "        \n",
    "        # Run YOLO detection\n",
    "        detections = yolo_model.detect(image)\n",
    "        \n",
    "        captions = []\n",
    "        for i, det in enumerate(detections):\n",
    "            x1, y1, x2, y2 = det['bbox']\n",
    "            # Add 10% padding to the bounding box\n",
    "            padding = 0.1\n",
    "            x1 = max(0, x1 - int(padding * (x2 - x1)))\n",
    "            x2 = min(image.shape[1], x2 + int(padding * (x2 - x1)))\n",
    "            y1 = max(0, y1 - int(padding * (y2 - y1)))\n",
    "            y2 = min(image.shape[0], y2 + int(padding * (y2 - y1)))\n",
    "\n",
    "            # Create a faded version of the image\n",
    "            faded_image = cv2.addWeighted(image, 0.2, np.zeros_like(image), 0.7, 0)\n",
    "\n",
    "            # Overlay the original bounding box region on the faded image\n",
    "            faded_image[y1:y2, x1:x2] = image[y1:y2, x1:x2]\n",
    "\n",
    "            # Convert the modified image to RGB for BLIP\n",
    "            faded_image_rgb = cv2.cvtColor(faded_image, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(faded_image_rgb)\n",
    "\n",
    "            # Generate caption using BLIP\n",
    "            caption = blip_pipeline(pil_image)[0]['generated_text']\n",
    "            captions.append({'bbox': det['bbox'], 'caption': caption})\n",
    "            \n",
    "            # Display or save the result for each detection\n",
    "            cv2.imshow(f\"Faded Image for Region {i + 1}\", faded_image)\n",
    "            cv2.waitKey(0)\n",
    "\n",
    "            # Optional: Save the image for reference\n",
    "            cv2.imwrite(f\"faded_region_{i + 1}.png\", faded_image)\n",
    "\n",
    "            print(f\"Region {i + 1}: {caption}\")\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "        return captions\n",
    "\n",
    "\n",
    "    # Usage Example\n",
    "    if __name__ == \"__main__\":\n",
    "        # Path to YOLO model\n",
    "        yolo_model_path = \"yolov8s-world.pt\"  # Replace with your YOLO model path\n",
    "        image_path = \"image.png\"  # Replace with your image path\n",
    "\n",
    "        # Initialize and load YOLO model\n",
    "        yolo = YOLO(model_path=yolo_model_path, device=\"cuda\", confidence_threshold=0.35)\n",
    "        yolo.load_model()\n",
    "\n",
    "        # Perform detection and BLIP captioning\n",
    "        results = fade_and_caption(image_path, yolo)\n",
    "        print(\"Captions for detected regions:\", results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
